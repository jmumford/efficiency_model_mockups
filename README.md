# efficiency_model_mockups

* A google slide deck [found here](https://docs.google.com/presentation/d/15qc8DHQ_8VCVIX6gASrjQIuLV7KIRNbVxNnqPzLIUC8/edit?usp=sharing) has a diagram illustrating the stimulus presentation structure for each task.  For some tasks, additional slides explain why we changed from the originally planned design or why we decided to ignore, for example, when a task contrast has slightly elevated VIFs.  Additionally, the last slide for each task shows statistics, compared to all 100,000 randomly generated designs, for the chosen 5 events files for that task, including efficiency, VIF, scan duration and some psychological assessment statistics.  
* `eff_task_name.ipynb` files: Show how designs were built and modified based on inspection of 250 randomly generated designs.  Model change decisions were based on on efficiency, VIF and task length for 250 random designs.  Once the settings were finalized, 100,000 designs were generated on Sherlock (code not included here nor is the full set of simulated designs)
* `utils.py`: Utility functions used within each of the eff_task_name.ipynb files.  Contains functions for calculating efficiency, VIFs, and some psychological fitness measures.  Note, the psychological fitness measures are still under development, but most seemed to work well in the generation and selection of these designs.  One measure aimed to predict trial ordering in the second half of the run based on the first half, but it failed to work and was not used in design selection.
* `choose_designs.ipynb`: This code is how the top 5 designs from each set of 100,000 were selected.  I ranked each measure (efficiency, VIF, scan length and psychological fitness measures) and selected the top X designs within each measures ranking.  Then designs that were within the top X for all measures were studied (i.e., designs in the intersection). The threshold, X, was adjusted until intersection contained the desired number of designs (5).  For cuedTS and spatialTS the study design was exactly the same and so a single code base was used to generate designs, which is why `cued_ts_spatial_ts` includes 10 designs instead of 5.  Similarly for `flanker_stroop`, which had the exact same study design.
* `final_designs`:  A directory containing the final designs for each task.  The events csv files in each sub directory are the events files.  The numeric value at the end of the filename simply refers to the index of the simulated design (after loading and merging into a single pandas data frame within the `choose_designs.ipynb` code).  The assessment_values.csv file shows the eff, VIF, etc for each chosen design.  Note, these are the values plotted with the histograms that are included in the google slide deck
* `calc_last_trial_offset.ipynb`: Calculates the offset of the last stimulus.  I did not add any extra time to this, but as much time as preferred can be added (I'd say at least 10s).  
    * The task run lengths from this code are output in `task_timing_info.csv` in the `final_designs` directory

The notebooks include useful figures, but the most important figures have been included in the google slide deck.