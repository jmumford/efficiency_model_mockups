# efficiency_model_mockups

This outlines various resources that were created when running this efficiency analysis. Briefly, a google slide deck was used to quickly summary task design, why some decisions were made and efficiency results.  The code in this repository was used to fine tune the designs and then a separate code base (`design_generation_code_sherlock`) was run on Sherlock to generate 100,000 random designs from which the `choose_designs.ipynb` code made the final selections.

* A google slide deck [found here](https://docs.google.com/presentation/d/15qc8DHQ_8VCVIX6gASrjQIuLV7KIRNbVxNnqPzLIUC8/edit?usp=sharing) has a diagram illustrating the stimulus presentation structure for each task.  For some tasks additional slides explain why we changed from the originally planned design or why we decided to ignore, for example, when a task contrast has slightly elevated VIFs.  Additionally, the last slide for each task shows model assessment statistics, compared to all 100,000 randomly generated designs, for the chosen 5 events files for that task, including efficiency, VIF, scan duration and some psychological assessment statistics.  
* `eff_<task_name>.ipynb` files (in `desiging_tasks`): Show how designs were built and modified based on inspection of 250 randomly generated designs.  Model change decisions were based on on efficiency, VIF and task length for 250 random designs.  Once the settings were finalized, 100,000 designs were generated on Sherlock (code not included here nor is the full set of simulated designs).  The Sherlock code can be found here:`/home/groups/russpold/rdoc_fmri/efficiency_analysis/`
* `image_files`: Stores png's of design diagrams that were used in the `designing_tasks` notebooks
* `utils.py`: Utility functions used within each of the `eff_task_name.ipynb` files.  Contains functions for calculating efficiency, VIFs, and some psychological fitness measures.  Note, the psychological fitness measures are still under development, but most seemed to work well in the generation and selection of these designs.  One measure aimed to predict trial ordering in the second half of the run based on the first half, but it failed to work and was not used in design selection.
* `design_generation_code_sherlock`: The code in this directory is what was run on Sherlock to generate the designs that were processed in `choose_designs.ipynb`.
* `choose_designs.ipynb`: This code is how the top 5 designs from each set of 100,000 were selected.  I ranked each measure (efficiency, VIF, scan length and psychological fitness measures) and selected the top X designs within each measures ranking.  Then designs that were within the top X for all measures were studied (i.e., designs in the intersection). The threshold, X, was adjusted until intersection contained the desired number of designs (5).  For cuedTS and spatialTS the study design was exactly the same and so a single code base was used to generate designs, which is why `cued_ts_spatial_ts` includes 10 designs instead of 5.  Similarly for `flanker_stroop`, which had the exact same study design.
* `check_vifs_new_alg.ipynb`: Partway through this process I idenfied issues with the original VIF calculation and this notebook compares the old vs repaired VIFs.  Due to the change all final design selection was based on the repaired VIF estimates.
* `check_nback.ipynb`: During design selection the nback selection mostly included designs where the first block was 2back.  This is because the VIF for the 2back vs 1back comparison was generally smaller for runs that started with 2back.  This is due to small changes in the covariance between the 2back and 1back regressors due to the "extra trials" regressor that models the extra trials at the beginning of a block.  
* `final_design_qa_figures.ipynb`: Contains figures to help check for unwanted patterns in trial orders and summaries of design metrics.  The purpose of these images is to help in selecing the final designs from the subset of designs identified in `choose_designs.ipynb`.
* `final_designs`:  A directory containing the final designs for each task.  The events csv files in each sub directory are the events files.  The numeric value at the end of the filename simply refers to the index of the simulated design (after loading and merging into a single pandas data frame within the `choose_designs.ipynb` code).  The assessment_values.csv file shows the eff, VIF, etc for each chosen design.  Note, these are the values plotted with the histograms that are included in the google slide deck

* `calc_last_trial_offset.ipynb`: Calculates the offset of the last stimulus.  I did not add any extra time to this, but as much time as preferred can be added (I'd say at least 10s).  
    * The task run lengths from this code are output in `task_timing_info.csv` in the `final_designs` directory

The notebooks have useful figures, but the most important figures have been included in the google slide deck for easier viewing.